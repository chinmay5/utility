{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    Needless to say, the notebook is incomplete. It does not have `classification_train_loader`, \n",
    "    `classification_test_loader`, `embedding_model`. However, the main idea of this file is just to ensure\n",
    "    that we get hold of the KNN in a fast manner\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us save the embedding of all elements in the train_data as well as their index\n",
    "# this can be used later for our purpose\n",
    "embedding_space = []\n",
    "for i, data in enumerate(classification_train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        batch_size = inputs.shape[0]\n",
    "        inputs = inputs.reshape(batch_size,-1)\n",
    "        labels = labels.cuda()\n",
    "        embeddings = embedding_model(inputs)\n",
    "        for index,tensor in enumerate(embeddings):\n",
    "            embedding_space.append((labels[index],tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    We should get hold of the tensor which is placed in the second\n",
    "    dimension of the tuple. Just put it in a list\n",
    "\"\"\"\n",
    "dist = [space for index,space in embedding_space[:-1]]\n",
    "\"\"\"\n",
    "    List of tensor handling is not easy and I often struggled with how exactly to \n",
    "    convert it into a matrix. Turns out, doing a simple `stack` was sufficient.\n",
    "    Now we have a matrix that we can work upon\n",
    "\"\"\"\n",
    "result = torch.stack(dist, dim=0)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Note how we move from the list of tuples to the distance embeddings.\n",
    "    Luckily, we have the same set of values at both the ends which is \n",
    "    really helpful and we can directly index them\n",
    "\"\"\"\n",
    "from collections import Counter\n",
    "def k_nearest_neighbour(distance_embeddings, vector, k=3):   \n",
    "    updated_result = torch.norm((distance_embeddings - vector),p=2,dim=1)\n",
    "    values, indices = torch.topk(updated_result,k=k ,largest=False)\n",
    "    candidate_indexes = []\n",
    "    for index in indices:\n",
    "        candidate_indexes.append((embedding_space[index][0]).item())\n",
    "    return Counter(candidate_indexes).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data in classification_test_loader:\n",
    "    inputs, labels = data\n",
    "    inputs = inputs.cuda()\n",
    "    batch_size = inputs.shape[0]\n",
    "    inputs = inputs.reshape(batch_size,-1)\n",
    "    # Next the evaluation\n",
    "    output = embedding_model.fc(inputs)\n",
    "    for idx,query in enumerate(output):\n",
    "        predicted = k_nearest_neighbour(result, query, 1)\n",
    "        correct += (predicted == labels[idx])\n",
    "    total += labels.size(0)\n",
    "    correct = correct.cpu().sum()\n",
    "print('Accuracy of the network on the test samples: %d %%' % (\n",
    "        100 * correct /total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
